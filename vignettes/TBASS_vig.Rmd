---
title: "TBASS: A Robust Adaptation of Bayesian Adaptive Spline Surfaces"
author: "Andy A. Shen, Kellin N. Rumsey, Devin C. Francom"
date: "Statistical Sciences Group (CCS-6): Los Alamos National Laboratory"
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
output: 
  pdf_document:
    number_sections: true
vignette: >
  %\VignetteIndexEntry{TBASS: A Robust Adaptation of Bayesian Multivariate Adaptive Regression Splines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = TRUE,
  comment = "# >"
)
```

```{r setup, include=FALSE}
library(TBASS)
# rmarkdown::html_vignette
```

------------------------

\begin{abstract}

The R package `TBASS` is an extension of the `BASS` package created by Francom et. al (2016). The package is used to fit a Bayesian   adaptive spline surface to a dataset that follows a Student's t-distribution or has outliers. Much of the framework for `TBASS` is adapted from the concepts of Bayesian Multivariate Adaptive Regression Splines (BMARS), specifically the work done from Denison, Mallick, and Smith (1998). By including a more robust generalization, a dataset with outliers can now be accurately fit using the BMARS model, without the possibility of overfitting or variance inflation.\newline

\end{abstract}

**Keywords**: splines, robust regression, Bayesian inference, nonparametric regression, sensitivity analysis


------------------------

# Introduction

Splines are a commonly used regression tool for fitting nonlinear data, univariate and multivariate. Splines can act as basis functions, where all of the basis functions combine to form the $\boldsymbol X$ matrix. The simplest way to create the ith basis functions can be represented as

\begin{equation}
\label{1}
X_{ij} = [s_i(x_{j} - t_{i})]_{+}
\end{equation}

Equation (\ref{1}) is used to calculate the ith column of the $X$ matrix of basis functions, where $s_i \in {-1,1}$, $t_i$ is called a **knot** and $[a]_{+} = max(0, a)$.

For example, given the nonlinear data shown in Figure 1 below, we can use (\ref{1}) to fit a spline model shown in Figure 2.

```{r,echo=FALSE}
set.seed(12)
n <- 300
x <- seq(0,1,length.out=n)
y <- sin(2 * pi * x^2) * 10 + rnorm(n)
```

```{r,echo=FALSE}
generate_spline <- function(tvec, nknot = length(tvec)) {
  s <- sample(c(1), nknot, replace = TRUE)
  Bmat <- matrix(NA, nknot, length(x))
  hs <- Bmat
  
  for(i in 1:nknot) {
    for(j in 1:length(x)) {
      Bmat[i,j] <- max(s[i] * (x[j] - tvec[i]), 0)
    } #creating basis functions
  }
  
  mBmat <- t(Bmat)
  mod <- lm(y ~ mBmat) #use gibbs to sample coefs in bayes
  pred <- predict(mod)

  
  plot(x,y,main=paste0("Figure ",2," : Univariate Spline Function"))
  lines(x, pred, type = "l", lwd = 5, col="blue1")
  #list(basis=mBmat)
}
t <- c(0, 0.525, 0.875)
```

```{r,echo=FALSE}
plot(x,y,main=paste0("Figure ",1," : Univariate Nonlinear Data"))
generate_spline(t)
```


\pagebreak

# Robust BMARS 

## Overview

We want to extend the theory behind frequentist univariate spline regression to a multivariate Bayesian framework. Moreover, we want to be able to fit nonlinear data that has outliers. We adopt the Gaussian BMARS framework based on the standard framework used by Dennison et al. (1998) and Francom (2018) when deriving our likelihood and full conditional distributions for the parameters. 

In the presence of outliers, Gaussian BMARS will attempt to capture the excess noise by adding basis functions (overfitting) or inflating the variance term ($\sigma^2$). The Robust BMARS model accounts for this sensitivity to outliers by avoiding overfitting or variance inflation when the degrees of freedom ($\nu$) are low. When $\nu$ is high, the t-distribution closely mimics a normal distribution, so the Robust BMARS model behaves in a similar way.

The function used to fit the Robust BMARS model in the `TBASS` package is the `tbass()` command (see section [3](#tbass)). 

## Auxiliary Variables

In creating the Robust BMARS model, we introduce new auxiliary parameters based on the work done by Gelman et al. (2014).  

Similar to Gaussian BMARS, we let $y_i$ be our dependent variable and $\mathbf{x}_i$ be our independent variable representing a single basis function with $i = 1,...,n$. Without loss of generality, all independent variables $\mathbf{x}_i$ are scaled from zero to one (Francom, 2016).

In the robust case, $y_i$ is modeled as

\begin{equation}
\label{2}
\boldsymbol {y} = X\boldsymbol{\beta} + \epsilon, \quad \epsilon \sim t_{\nu}\left(0, ~ \sigma^2 {V}^{-1}\right)
\end{equation}

and

\begin{equation}
\label{3}
y_i | V_i \sim \mathcal{N}\left(X\boldsymbol{\beta}, ~ \frac{\sigma^2}{V_i}\right)
\end{equation}

where $\boldsymbol X$ represents the matrix of basis functions by column, $\boldsymbol \beta$ is the vector of regression coefficients, $\epsilon$ is the error term,  $\nu$ represents the degrees of freedom in a Student's t-distribubtion, $\sigma^2$ is the variance term for $y_i$, and $V^{-1} = diag(1/V_1...1/V_n)$ where $V_i$ is the variance estimate of $y_i$.

The basis functions themselves are produced the same way as in the `BASS` package by Francom, et al. (2016).

## Priors

We assume an Inverse-Gamma prior for $\sigma^2$ with default shape $\gamma_1 = 0$ and default rate $\gamma_2=0$, and a Gamma prior with shape and rate $\frac{\nu}{2}$ for $V_i$, such that

\begin{equation}
\label{4}
\sigma^2 \sim IG~(\gamma_1,~\gamma_2)
\end{equation}

\begin{equation}
\label{5}
V_i \sim \Gamma \bigg(\frac{\nu}{2}, ~\frac{\nu}{2}\bigg)
\end{equation}

From there, we obtain the full conditional of $V_i$ as

\begin{equation}
\label{6}
V_i | \cdot \sim \Gamma\left\lbrace\frac{\nu + 1}{2},~ \frac{1}{2\sigma^2} \sum_{i=1}^{n}{(y - X\beta)^2}\right\rbrace
\end{equation}

For the parameter governing the number of basis functions $\lambda$, we have that 

\begin{equation}
\label{7}
\lambda | \cdot \sim \Gamma\left( h_1 + M,~h_2+1\right)
\end{equation}

where $h_1 = h_2= 10$ are the default hyperparameters for $\lambda$ and $M$ is the current number of basis functions.

It follows that $\lambda$ follows a gamma prior.
 
## Regression Coefficients

Finally, our regression coefficients $\boldsymbol\beta$ follow a Gaussian prior such that

\begin{equation}
\label{8}
\boldsymbol\beta | \cdot \sim \mathcal{N} \left( 0,~\tau^2 {I} \right)
\end{equation}


In the Student's t-distribution, we can marginalize the posterior for $\boldsymbol\beta$ and $\sigma^2$ to obtain the regression estimate $\boldsymbol{\hat{\beta}}$:

\begin{equation}
\label{9}
\boldsymbol{\hat{\beta}} = 
\frac{1}{\sigma^2}
\left( \frac{1}{\sigma^2} {X'VX} + {\tau^{-2}}{{I}} \right)^{-1} {X'}V{{y}}
\end{equation}


where $\tau^2$ is the prior variance for $\beta_{i}$.

## Likelihood Function

Estimate (\ref{9}) allows us to achieve our t-distributed likelihood function for the birth step. After marginalizing out $\boldsymbol\beta$ and $\sigma^2$, we can simplify our Likelihood $L$ to

$$
L \propto \left(\tau^2\right)^{-\frac{M+1}{2}}~ |V|^{-1/2}~ \left|H^{-1}\right|^{-1/2}exp\left(-\frac{1}{2}y'V^{-1}y \right) exp\left(-\frac{1}{2}\boldsymbol\beta'H^{-1}\boldsymbol\beta - 2\boldsymbol\beta'X'V^{-1}y\right)
$$

where $H = (X'V^{-1}X + \tau^{-2}I)$.\newline

We then complete the square on the first exponential term using $y~'V^{-1}X~ H^{-1}~X'V^{-1}y $ which simplifies to $\boldsymbol{\hat{\beta}}' H \boldsymbol{\hat\beta}$ since $X'V^{-1}y = H\hat\beta$ from (\ref{9}).\newline

From there, we have that

$$
\begin{aligned}
L(M) &\propto \left(2\pi\tau^2\right)^{-\frac{M+1}{2}}~ |V|^{-1/2}~exp\left(-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}' H \boldsymbol{\hat\beta}\right)\right)
exp\left(-\frac{1}{2}\left(\boldsymbol\beta'H\boldsymbol\beta - \boldsymbol\beta'H\boldsymbol{\hat{\beta}} - \boldsymbol{\hat{\beta}}'H\boldsymbol\beta + \boldsymbol{\hat{\beta}}'H\boldsymbol{\hat{\beta}} \right)\right)\\
&\propto \left(2\pi\tau^2\right)^{-\frac{M+1}{2}}~ |V|^{-1/2}~exp\left(-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}' H \boldsymbol{\hat\beta}\right)\right)
exp\left\lbrace-\frac{1}{2}\left[\left(\boldsymbol\beta -\boldsymbol{\hat{\beta}} \right)'H\left(\boldsymbol\beta -\boldsymbol{\hat{\beta}}\right)\right]\right\rbrace\\
&\propto \left(2\pi\tau^2\right)^{-\frac{M+1}{2}}~ |V|^{-1/2}~ exp\left(-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}' H \boldsymbol{\hat\beta}\right)\right)~
\mathcal N\left(\boldsymbol\beta \mid \boldsymbol{\hat\beta}, ~H^{-1}\right) \left|H^{-1}\right|^{-1/2}\left(2\pi\right)^{\frac{M+1}{2}}\\
&\propto \left(\tau^2\right)^{-\frac{M+1}{2}}  |V|^{-1/2}~ \left|H^{-1}\right|^{-1/2} exp\left\lbrace-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}' H \boldsymbol{\hat\beta}\right)\right\rbrace\\
&\propto \left(\tau^2\right)^{-\frac{M+1}{2}}  |V|^{-1/2}~ \left|H^{-1}\right|^{-1/2} exp\left\lbrace-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}' \left(X'V^{-1}X + \tau^{-2}I\right) \boldsymbol{\hat\beta}\right)\right\rbrace
\end{aligned} 
$$

\begin{equation}
\label{like}
\Rightarrow L(M) = (\tau^2)^{\frac{M+1}{2}} |V|^{-1/2} ~\left|(X^tV^{-1}X +{\tau^{-2}} I)^{-1}\right|^{-1/2} exp\bigg\lbrace -\frac{1}{2}\big(y'V^{-1}y - \boldsymbol{\hat\beta}^t (X^tV^{-1}X +{\tau^{-2}} I)^{-1}\boldsymbol{\hat\beta}\big) \bigg\rbrace
\end{equation}

Equation (\ref{like}) is the likelihood for the current state model with $M$ basis functions.\newline


## Reversible-Jump Markov Chain Monte Carlo (RJ-MCMC)

Like Gaussian BMARS, the robust algorithm builds basis functions adaptively, sampling from candidate knot locations, signs, interaction degrees, and accepting or rejecting the basis functions using a RJ-MCMC algorithm to sample from the full posterior.

RJ-MCMC is an generalization of the traditional Metropolis-Hastings algorithm in the sense that RJ-MCMC allows for parameter dimension change, allowing for simulation when the number of parameters is unknown. This is important for BMARS because we want to learn where the knots should be placed and how many basis functions to have in our model, along with the degree of interaction for our basis functions. There can exist multiple basis functions in a multivariate setting. We also want to know if certain basis functions should be added, deleted, or changed.

The BMARS model has three possible move types, which are sampled using a discrete uniform:

  - **Birth**: adding a basis function 

  - **Death**: deleting a basis function

  - **Change**: changing a knot, sign, and values of a basis function 

Once the move type is sampled, the RJ-MCMC algorithm is used to determine acceptance of that move type.

Our acceptance ratio $\alpha$ is denoted by

\begin{equation}
\label{TBD}
\alpha = min\left\lbrace 
1,\
\frac{L(D|\theta') \ p(\theta')\ S(\theta'\rightarrow\theta)}{L(D|\theta)\ p(\theta) \ S(\theta\rightarrow\theta')} 
\right\rbrace
\end{equation}

where $\theta'$ represents the candidate model parameters and $\theta$ represent the current state model parameters, $L$ is the Gaussian likelihood, $p$ is the prior, and $S$ is the proposal to jump from one model to another. 

Section [2.7](#birth) details the RJ-MCMC algorithm for the birth step in detail, and the death and change steps are very similar.


## Birth Step {#birth}

Since a basis function is added for the birth step, we have that


\begin{equation}
L(M+1) \propto 
\left(\tau^2\right)^{-\frac{M+2}{2}}  |V|^{-1/2}~ \left|H^{-1}\right|^{-1/2} exp\left\lbrace-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}' \left(X'V^{-1}X + \tau^{-2}I\right) \boldsymbol{\hat\beta}\right)\right\rbrace
\end{equation} 


Using (\ref{like}), The ratio $\frac{L(M+1)}{L(M)}$ is then represented as


\begin{equation}
\label{12}
\frac{L(M+1)}{L(M)} = \frac{\left(\tau^2\right)^{-\frac{M+2}{2}}  |V|^{-1/2}~ \left|H^{-1}_c\right|^{-1/2} exp\left\lbrace-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}'_{c} \left(X'_{c}V^{-1}X_{c} + \tau^{-2}I_{c}\right) \boldsymbol{\hat\beta}_{c}\right)\right\rbrace}
{\left(\tau^2\right)^{-\frac{M+1}{2}}  |V|^{-1/2}~ \left|H^{-1}\right|^{-1/2} exp\left\lbrace-\frac{1}{2}\left(y'V^{-1}y - \boldsymbol{\hat{\beta}}' \left(X'V^{-1}X + \tau^{-2}I\right) \boldsymbol{\hat\beta}\right)\right\rbrace}\\\\
\end{equation}

where the subscript $c$ denotes the candidate move type of the algorithm (a birth step in this case).\newline

The death is very similar to the birth step, with a candidate likelihood of $L(M-1)$, and there is no dimension change in the change step.

The RJ-MCMC algorithm calculates the likelihoods, priors, proposals, and acceptance ratios all on the log scale.

## Gibbs Sampling 

Once the basis function move type is complete, the model parameter values $\lambda$, $\boldsymbol{\beta}$, $V_i$ and $\sigma^2$ can then be sampled using Gibbs Sampling, since the full conditionals are all closed-form.

The full conditionals are of the following form:

\begin{equation}
\lambda | \cdot\sim \Gamma\big(h_1 + M, ~h_2 + 1\big)
\end{equation}

\begin{equation}
\boldsymbol{\beta} | \cdot \sim \mathcal{N}\bigg( \boldsymbol{\hat\beta}, \left(X'V^{-1}X + \tau^{-2}I\right) \bigg)
\end{equation}

where $\boldsymbol{\hat\beta}$ is denoted in (\ref{9}).\newline

\begin{equation}
V_i | \cdot \sim \Gamma\left\lbrace\frac{\nu + 1}{2},~ \frac{1}{2\sigma^2} \sum_{i=1}^{n}{(y - X\beta)^2}\right\rbrace
\end{equation}

\begin{equation}
\sigma^2 | \cdot\sim IG\bigg(\gamma_1 + \frac{n}{2}, ~\gamma_2 + \frac{1}{2} (\boldsymbol y - \boldsymbol{X\beta})^t (\boldsymbol y - \boldsymbol{X\beta})  \bigg)
\end{equation}

\pagebreak

# Simulation with `tbass()` {#tbass}

We now demonstrate the capabilities of the `TBASS` package using the main command, `tbass()`. For all parameter values of this function, please refer to the help documentation by running `?tbass` after loading the package.\newline

**NOTE**: At this time, the package `mnormt` is REQUIRED in order to use `TBASS`. Please run `install.packages("mnormt")` to install the package. This dependency will be removed when the package is updated further. No other dependencies are required.\newline

We begin by loading in the package and setting the seed for reproducibility. The package can be installed using the following command: `devtools::install_github("aashen12/TBASS")`.\newline

```{r,echo=TRUE}
set.seed(12)
library(TBASS)
```


## Friedman Function

Our first example fits the Robust BMARS model to the infamous Friedman Function (Friedman, 1991):

\begin{equation}
\label{fried}
10 ~sin(  \pi x_1 x_2) ~+~ 20(x_3 - 0.5)^2 ~+~ 10x_4 ~+~ 5x_5 
\end{equation}

In R:\newline

```{r,echo=TRUE}
f <- function(x){
  10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2 + 10*x[,4] + 5*x[,5]
} 
```

We set our true value of $\sigma = 1$ and attempt to capture the same variability in Robust BMARS\newline

```{r, echo=TRUE}
sigma <- 1 # TRUE noise sd
n <- 1000 # number of observations
x <- matrix(runif(n*5), nrow = n, ncol = 5)
y <- rnorm(n, mean = f(x), sd = sigma)
```

We then add extra noise to simulate a dataset with outliers, and categorize them for plotting later. \newline

```{r}
ind <- sample(n,size=10) # convert 10 points to outliers
y[ind] <- rnorm(5, f(x[ind,]), 15)

col <- rep(1,n) # for coloring these points in a later plot
col[ind] <- 2
```

From there, we can run the `tbass()` command with 10000 MCMC iterations. Our first simulation is with $\nu = 10$ degrees of freedom to simulate a t-distribution with thicker tails.\newline

```{r,cache=TRUE}
nmcmc <- 10000 #number of iterations
tb <- tbass(x, y, nu = 10, nmcmc = nmcmc, verbose = FALSE)
```


## Results


### Overfitting and Prediction

We begin by plotting the number of basis functions throughout the entire simulation.

```{r}
fig <- 3
plot(tb$nbasis, type = "l",
     main = paste0("Figure ", fig, ": Trace plot of number of basis functions"))
```

We see that there are 10 basis functions, indicating that there was no overfitting in the `TBASS` model.

We then assess the accuracy of our predicted y values ($X\beta$) vs the actual y-values.

```{r}
fig <- fig + 1
plot(tb$X %*% tb$b, y, col = col,
main=paste0("Figure ", fig, ": Predicted vs Actual Values"), 
xlab = "Predicted y-values")
abline(0,1,col="blue2",lwd=2.5)
```

We see that the prediction is quite accurate with respect to the outliers.

### Gibbs Results

We set a burn-in value to the first 10% of samples.

```{r}
burn_final <- nmcmc/10
burn <- 1:burn_final
```

From there, we plot our $\sigma$ values to ensure there was no variance inflation.

```{r}
fig <- fig + 1
plot(tb$s2[-burn], type = "l",
    main = paste0("Figure ", fig, ": TBASS Variance Trace Plot"))
```

From this plot, we see that our simulated $\sigma^2$ values are close to $1$, which matches our true value of $\sigma^2$ when we generated our data.

We then plot our $\frac{1}{V_i}$ values to see if the outliers are accounted for. We thin every 100 iterations after the burn-in.

```{r}
fig <- fig + 1
matplot(sqrt(1/tb$v[seq(burn_final,nmcmc,100),]), type="l",
  main = paste0("Figure ",fig, ": Plot of V_i"))
```

We see that the peaks in $V_i$ are accounting for the outliers, while the majority of $V_i$ values are in the bottom portion of the plot.

## Comparison with `BASS`

To account for the differences in behavior for Robust BMARS (`TBASS`) and Gaussian BMARS (`BASS`), we compare our results from TBASS to the output from BASS, which assumes a Gaussian likelihood.

```{r, cache = TRUE}
library(BASS)
b <- bass(x,y) # automatically runs 10000 nmcmc iterations
```

### BASS Basis Functions

```{r}
fig <- fig + 1
plot(b$nbasis, type = "l",
     main = paste0("Figure ", fig, ": BASS Basis Function Count"))
```


### BASS Variance

```{r}
fig <- fig + 1
plot(b$s2, type = "l",
     main = paste0("Figure ", fig, ": BASS Variance Trace Plot"))
```

We see that the $\sigma^2$ from Gaussian BMARS is over two times as large as the $\sigma^2$ from `TBASS`, indicating that Gaussian BMARS is more sensitive to outliers and will increase $\sigma^2$ when outliers are present.  






