---
title: "TBASS: A Robust Adaptation of Bayesian Adaptive Spline Surfaces"
author: "Andy A. Shen, Kellin N. Rumsey, Devin C. Francom"
date: "Statistical Sciences Group (CCS-6): Los Alamos National Laboratory"
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
output: 
  pdf_document:
    number_sections: true
vignette: >
  %\VignetteIndexEntry{TBASS: A Robust Adaptation of Bayesian Multivariate Adaptive Regression Splines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = FALSE,
  comment = "#>"
)
```

```{r setup, include=FALSE}
library(TBASS)
# rmarkdown::html_vignette
fig <- 1
```

------------------------

\begin{abstract}

The R package `TBASS` is an extension of the `BASS` package created by Francom et. al (2016). The package is used to fit a Bayesian   adaptive spline surface to a dataset that follows a Student's t-distribution or has outliers. Much of the framework for `TBASS` is adapted from the concepts of Bayesian Multivariate Adaptive Regression Splines (BMARS), specifically the work done from Denison, Mallick, and Smith (1998). By including a more robust generalization, a dataset with outliers can now be accurately fit using the BMARS model, without the possibility of overfitting or variance inflation.\newline

\end{abstract}

**Keywords**: splines, robust regression, Bayesian inference, nonparametric regression, sensitivity analysis


------------------------

# Introduction

Splines are a commonly used regression tool for fitting nonlinear data. Splines can act as basis functions, where all of the basis combine to form the $\boldsymbol X$ matrix. The simplest way to create the ith basis functions can be represented as

\begin{equation}
\label{1}
X_{ij} = [s_i(x_{j} - t_{i})]_{+}
\end{equation}

Equation (\ref{1}) is used to calculate the ith column of the $X$ matrix of basis functions, where $s_i \in {-1,1}$, $t_i$ is called a **knot** and $[a]_{+} = max(0, a)$.

For example, given the nonlinear data shown in Figure 1 below, we can use (\ref{1}) to fit a spline model shown in Figure 2.

```{r}
n <- 300
x <- seq(0,1,length.out=n)
y <- sin(2 * pi * x^2) * 10 + rnorm(n)
plot(x,y,main=paste0("Figure ",fig," : Univariate Nonlinear Data"))
fig <- fig + 1
```

\  

```{r}
generate_spline <- function(tvec, nknot = length(tvec)) {
  s <- sample(c(1), nknot, replace = TRUE)
  Bmat <- matrix(NA, nknot, length(x))
  hs <- Bmat
  
  for(i in 1:nknot) {
    for(j in 1:length(x)) {
      Bmat[i,j] <- max(s[i] * (x[j] - tvec[i]), 0)
    } #creating basis functions
  }
  
  mBmat <- t(Bmat)
  mod <- lm(y ~ mBmat) #use gibbs to sample coefs in bayes
  pred <- predict(mod)

  
  plot(x,y,main=paste0("Figure ",fig," : Univariate Spline Function"))
  lines(x, pred, type = "l", lwd = 5, col="blue1")
  #list(basis=mBmat)
}
t <- c(0, 0.525, 0.875); generate_spline(t)
```


\pagebreak

# Robust BMARS 

## Overview

We want to extend the theory behind frequentist univariate spline regression to a multivariate Bayesian framework. Moreover, we want to be able to fit nonlinear data that has outliers. We adopt the Gaussian BMARS framework based on the standard framework used by Dennison et al. (1998) and Francom (2018) when deriving our likelihood and full conditional distributions for the parameters. 

In the presence of outliers, Gaussian BMARS will attempt to capture the excess noise by adding basis functions (overfitting) or inflating the variance term ($\sigma^2$). The Robust BMARS model accounts for this sensitivity to outliers by avoiding overfitting or variance inflation when the degrees of freedom ($\nu$) are low. When $\nu$ is high, the t-distribution closely mimics a normal distribution, so the Robust BMARS model behaves in a similar way.

The function used to fit the Robust BMARS model in the `TBASS` package is the `tbass()` command (see section [4](#tbass)). 

## Auxiliary Variables

In creating the Robust BMARS model, we introduce new auxiliary parameters based on the work done by Gelman et al. (2014).  

Similar to Gaussian BMARS, we let $y_i$ be our dependent variable and $\mathbf{x}_i$ be our independent variable representing a single basis function with $i = 1,...,n$. Without loss of generality, all independent variables $\mathbf{x}_i$ are scaled from zero to one (Francom, 2016).

In the robust case, $y_i$ is modeled as

\begin{equation}
\label{2}
\boldsymbol {y} = \boldsymbol{X\beta} + \epsilon, \quad \epsilon \sim t_{\nu}\left(0, ~ \sigma^2 \boldsymbol{V}^{-1}\right)
\end{equation}

and

\begin{equation}
\label{3}
y_i | V_i \sim \mathcal{N}\left(\boldsymbol{X\beta}, ~ \frac{\sigma^2}{V_i}\right)
\end{equation}

where $\boldsymbol X$ represents the matrix of basis functions by column, $\boldsymbol \beta$ is the vector of regression coefficients, $\epsilon$ is the error term,  $\nu$ represents the degrees of freedom in a Student's t-distribubtion, $\sigma^2$ is the variance term for $y_i$, and $V^{-1} = diag(1/V_1...1/V_n)$ where $V_i$ is the variance estimate of $y_i$.

The basis functions themselves are produced the same way as in the `BASS` package by Francom, et al. (2016).

## Priors

We assume an Inverse-Gamma prior for $\sigma^2$ with default shape $\gamma_1 = 0$ and default rate $\gamma_2=0$, and a Gamma prior with shape and rate $\frac{\nu}{2}$ for $V_i$, such that

\begin{equation}
\label{4}
\sigma^2 \sim IG~(\gamma_1,~\gamma_2)
\end{equation}

\begin{equation}
\label{5}
V_i \sim \Gamma \bigg(\frac{\nu}{2}, ~\frac{\nu}{2}\bigg)
\end{equation}

From there, we obtain the full conditional of $V_i$ as

\begin{equation}
\label{6}
V_i | \cdot \sim \Gamma\left\lbrace\frac{\nu + 1}{2},~ \frac{1}{2\sigma^2} \sum_{i=1}^{n}{(y - X\beta)^2}\right\rbrace
\end{equation}

For the parameter governing the number of basis functions $\lambda$, we have that 

\begin{equation}
\label{7}
\lambda | \cdot \sim \Gamma\left( h_1 + M,~h_2+1\right)
\end{equation}

where $h_1 = h_2= 10$ are the default hyperparameters for $\lambda$ and $M$ is the current number of basis functions.

It follows that $\lambda$ follows a gamma prior.
 
## Regression Coefficients

Finally, our regression coefficients $\boldsymbol\beta$ follow a Gaussian prior such that

\begin{equation}
\label{8}
\boldsymbol\beta | \cdot \sim \mathcal{N} \left( 0,~\tau^2 \boldsymbol{I} \right)
\end{equation}


In the Student's t-distribution, we can marginalize the posterior for $\boldsymbol\beta$ and $\sigma^2$ to obtain the regression estimate $\boldsymbol{\hat{\beta}}$:

\begin{equation}
\label{9}
\boldsymbol{\hat{\beta}} = 
\frac{1}{\sigma^2}
\left( \frac{1}{\sigma^2} \boldsymbol{X'VX} + {\tau^{-2}}{\boldsymbol{I}} \right)^{-1} \boldsymbol{X' V}{\boldsymbol{y}}
\end{equation}


where $\tau^2$ is the prior variance for $\beta_{i}$.


## Reversible-Jump Markov Chain Monte Carlo (RJ-MCMC)

Like Gaussian BMARS, the robust algorithm builds basis functions adaptively, sampling from candidate knot locations, signs, interaction degrees, and accepting or rejecting the basis functions using a RJ-MCMC algorithm to sample from the full posterior.

RJ-MCMC is an generalization of the traditional Metropolis-Hastings algorithm in the sense that RJ-MCMC allows for parameter dimension change, allowing for simulation when the number of parameters is unknown. This is important for BMARS because we want to learn where the knots should be placed and how many basis functions to have in our model, along with the degree of interaction for our basis functions. There can exist multiple basis functions in a multivariate setting. We also want to know if certain basis functions should be added, deleted, or changed.

The BMARS model has three possible move types, which are sampled using a discrete uniform:

  - **Birth**: adding a basis function 

  - **Death**: deleting a basis function

  - **Change**: changing a knot, sign, and values of a basis function 

Once the move type is sampled, the RJ-MCMC algorithm is used to determine acceptance of that move type.

Our acceptance ratio $\alpha$ is denoted by

\begin{equation}
\label{TBD}
\alpha = min\left\lbrace 
1,\
\frac{L(D|\theta') \ p(\theta')\ S(\theta'\rightarrow\theta)}{L(D|\theta)\ p(\theta) \ S(\theta\rightarrow\theta')} 
\right\rbrace
\end{equation}

where $\theta'$ represents the candidate model parameters and $\theta$ represent the current state model parameters, $L$ is the Gaussian likelihood, $p$ is the prior, and $S$ is the proposal to jump from one model to another. 

Section [3](#birth) details the RJ-MCMC algorithm for the birth step in detail, and the death and change steps are very similar.

## Gibbs Sampling 

Once the basis function move type is complete, the model parameter values can then be sampled using Gibbs Sampling, since the full conditionals are all closed-form.

\pagebreak

# Birth Step {#birth}

## Likelihood Function

Estimate (\ref{9}) allows us to achieve our t-distributed likelihood function for the birth step: 

\begin{equation}
\label{10}
(\tau^2)^{\frac{M+1}{2}} |V|^{-1/2} ~\left|(X^tW^{-1}X +{\tau^{-2}} I)^{-1}\right|^{-1/2} exp\bigg\lbrace -\frac{1}{2}\big(y'V^{-1}y - \boldsymbol{\hat\beta}^t (X^tW^{-1}X +{\tau^{-2}} I)^{-1}\boldsymbol{\hat\beta}\big) \bigg\rbrace
\end{equation}

\begin{proof}
After marginalizing out \boldsymbol\beta and \sigma^2, we have that our Likelihood L \propto exp\left( \right)

\end{proof}

\pagebreak

# Simulation with `tbass()` {#tbass}

We now demonstrate the capabilities of the `TBASS` package using the main command, `tbass()`. For all parameter values of this function, please refer to the help documentation by running `?tbass` after loading the package.\newline

**NOTE**: At this time, the package `mnormt` is REQUIRED in order to use `TBASS`. Please run `install.packages("mnormt")` to install the package. This dependency will be removed when the package is updated further. No other dependencies are required.\newline

We begin by loading in the package and setting the seed for reproducibility. The package can be installed using the following command: `devtools::install_github("aashen12/TBASS")`.\newline

```{r,echo=TRUE}
set.seed(12)
library(TBASS)
```


## Friedman Function

Our first example fits the Robust BMARS model to the infamous Friedman Function (reference here):

\begin{equation}
\label{???}
10 ~sin( x_1 ~x_2~ \pi) ~+~ 20(x_3 - 0.5)^2 + 10x_4 + 5x_5 
\end{equation}



